{
  "hash": "29d04561ad45cad66aa40a83ac44ce86",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: Lecture lab\nsubtitle: Implementation of MCMC- with the Metropolis algorithm.\nengine: julia\n---\n\nWe will follow the steps described in the excellent article in introduction to the MCMC of David W. Hogg and Daniel Foreman-Mackey, available [here](https://ui.adsabs.harvard.edu/abs/2018ApJS..236...11H/abstract).\nSection 3 will be particularly useful for this example.\n\n\n## One-dimensional density function (Problems 2 and 3 of the article)\n### Gaussian density\n\nFor this first exercise, we will implement the Metropolis algorithm and apply it to a unidimensional normal distribution.\n\nUse the following information:\n\n- The density function $P(\\Theta)$ is a Gaussian with a dimension with average of $μ = 2 $ and a variance $σ^2 = 2 $.\n- The distribution of proposal $q (θ' | \\Theta) $ is a Gaussian for $θ'$ with an average $μ = \\Theta $ and a standard deviation $σ = 1$.\n- The initial point of the MCMC is $ \\Theta = 0 $.\n- The MCMC must perform $10^4 $ iterations.\n\nThe equation of normal distribution is\n\n$$\np(θ) = \\frac{1}{\\sqrt{2 \\pi σ^2}} \\exp\\left[ -\\frac{(θ - μ)^2}{2 σ^2}\\right].\n$$\n\nHowever, to avoid digital errors, you use your logarithm. **Code directly a function for $\\ln p(θ)$**.\n\n::: {#2 .cell execution_count=1}\n``` {.julia .cell-code}\nfunction log_gaussian(x, μ, σ)\n    -0.5 * ((x - μ) / σ)^2 + log(1 / (sqrt(2 * π) * σ))\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nlog_gaussian (generic function with 1 method)\n```\n:::\n:::\n\n\n\nWe can now implement the Metropolis algorithm.\n\nWe want our algorithm to be applicable to any density of (log-) probability which accepts an argument $ \\ Theta $ scalar. We can therefore give `log_Density` (our above probability function) in the function argument.\n\n::: {#4 .cell execution_count=1}\n``` {.julia .cell-code}\n\"\"\"\nMetropolis MCMC algorithm for sampling from a probability distribution.\n\nArguments:\n- log_density: log-density function, accepts a theta argument\n- θ₀: initial value of theta for the MCMC\n- n: number of steps to take in the MCMC\n- q_scale: standard deviation of the proposal distribution.\n\"\"\"\nfunction mcmc_metropolis(log_density, θ₀, n, q_scale=1.0, verbose=false)\n    chain = zeros(n + 1)\n    chain[1] = θ₀\n    p_c = log_density(θ₀)\n\n    n_accepted = 0\n    for i in 1:n\n        proposal = chain[i] + q_scale * randn()\n        p_i = log_density(proposal)\n        log_alpha = p_i - p_c\n\n        if log(rand()) < log_alpha # Accept the proposal\n            chain[i+1] = proposal\n            p_c = p_i\n            n_accepted += 1\n        else # Reject the proposal, stay at current position\n            chain[i+1] = chain[i]\n        end\n    end\n\n    # Print acceptance rate for diagnostics\n    verbose && println(\"Acceptance rate: $(n_accepted / n)\")\n\n    return chain\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nMain.Notebook.mcmc_metropolis\n```\n:::\n:::\n\n\n\nApply the algorithm to obtain 10,000 samples.\n\nThen display a histogram and compare it with the analytical PDF.\nThen display the temporal evolution ($Θ$ vs $k$) of the MCMC.\n\n::: {#6 .cell execution_count=1}\n``` {.julia .cell-code}\nusing CairoMakie\n\nμ = 2.0\nσ = sqrt(2.0)\nlog_density(θ) = log_gaussian(θ, μ, σ)\n\n# Initial conditions\nθ₀ = 0.0\nnsteps = 10000\nq_scale = 1.0\n\n# Run the MCMC algorithm\nsamples = mcmc_metropolis(log_density, θ₀, nsteps, q_scale)\n\n# Create a figure for the histogram and analytical PDF comparison\nf = Figure()\nax1 = Axis(f[1, 1], xlabel=\"θ\", ylabel=\"Density\", title=\"MCMC Sampling vs Analytical PDF\")\n\nhist!(ax1, samples, bins=50, normalization=:pdf, color=(:blue, 0.5), label=\"MCMC Samples\")\n\n# Overlay the analytical PDF\nθ_range = range(extrema(samples)..., length=1000)\nanalytical_pdf = [exp(log_density(θ)) for θ in θ_range]\nlines!(ax1, θ_range, analytical_pdf, color=:red, linewidth=2, label=\"Analytical PDF\")\n\naxislegend(ax1)\nf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](mcmc_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\n\n::: {#8 .cell execution_count=1}\n``` {.julia .cell-code}\nax2 = Axis(f[2, 1], xlabel=\"Iteration (k)\", ylabel=\"θ\", title=\"MCMC Trace Plot\")\nlines!(ax2, 0:nsteps, samples)\nf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](mcmc_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\n\n<!-- ```{julia}\nburnin = 1000  # Discard the first 1000 samples as burn-in\nsamples_post_burnin = samples[burnin+1:end]\n\nfunction autocorrelation(x, lags)\n    n = length(x)\n    x_centered = x .- mean(x)\n    var_x = sum(x_centered .^ 2) / n\n\n    ac = zeros(length(lags))\n    for (i, lag) in enumerate(lags)\n        ac[i] = sum(x_centered[1:n-lag] .* x_centered[lag+1:n]) / ((n - lag) * var_x)\n    end\n\n    return ac\nend\n\nlags = 0:100\nac = autocorrelation(samples_post_burnin, lags)\n\nax3 = Axis(f[3, 1],\n    xlabel=\"Lag\",\n    ylabel=\"Autocorrelation\",\n    title=\"Autocorrelation Plot\")\n\nlines!(ax3, lags, ac)\nf\n``` -->\n\n",
    "supporting": [
      "mcmc_files"
    ],
    "filters": [],
    "includes": {}
  }
}