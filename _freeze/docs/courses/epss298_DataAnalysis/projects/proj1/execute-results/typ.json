{
  "hash": "17c8ccb035f79c1f60d2eeba6bab5a6a",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: Project 1\nsubtitle: Transiting Exoplanet\nengine: julia\n---\n\n> Your team detected a planet candidate in the data from the Transiting Exoplanet Survey Satellite (TESS). The data is very noisy, but you are relatively confident the planet is there.  \n> Based on the TESS data, you can predict that the center of the next transit will be between the  time t = 1.212 days and t = 1.362 days [Note: t is measured relative to an arbitrary reference time]. The TESS data also tell you that the radius of the planet is between 1% and 10% the radius of the star. Unfortunately, you don't know where the planet crosses the star, i.e. the impact  parameter is unconstrained between 0 and 1. Assume that the orbital period is exactly known to be 3.0 days. \n> You wrote a proposal and successfully convinced the time allocation committee of the James Webb Space Telescope (JWST) to obtain new very precise data for you. YAY!!! \n> You receive the data in Project1_JWST_data.csv. The errors are 10-4=0.01% on each data point in the light curve. \n> From these data, you want to determine new estimates with uncertainties for the transit time, the planet-to-star radius ratio, and the impact parameter. \n\n> Batman: https://lkreidberg.github.io/batman/docs/html/index.html \n> [joshspeagle/dynesty: Dynamic Nested Sampling package for computing Bayesian posteriors and evidences](https://github.com/joshspeagle/dynesty)\n\nJulia packages:\n\n> [madsjulia/AffineInvariantMCMC.jl: Affine Invariant Markov Chain Monte Carlo (MCMC) Ensemble sampler](https://github.com/madsjulia/AffineInvariantMCMC.jl)\n> [chalk-lab/NestedSamplers.jl: Implementations of single and multi-ellipsoid nested sampling](https://github.com/chalk-lab/NestedSamplers.jl)\n> [bat/BAT.jl: A Bayesian Analysis Toolkit in Julia](https://github.com/bat/BAT.jl)\n\n::: {.callout-note}\nIn the updated version of the project, we use `Transits.jl` instead of `batman`, which is more performant and provides features like automatic differentiation.\n:::\n\n## Introduction\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\ndir = \"docs/courses/epss298_DataAnalysis\"\nif isdir(dir)\n    cd(dir)\n    Pkg.activate(\".\")\n    Pkg.instantiate()\n    cd(\"projects\")\nend\n```\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing CSV, DataFrames\nusing CairoMakie\npath = \"Project1_JWST_data.csv\"\ndata = CSV.read(path, DataFrame)\n\nf = Figure()\nmarkersize = 4\nax = Axis(f[1, 1], xlabel=\"Time\", ylabel=\"Flux\")\nscatterlines!(ax, data.time, data.flux; markersize)\nf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-3-output-1.svg){}\n:::\n:::\n\n\n\n## Part A\n \n### Step 1: Define the physical model \n\n> Write a function for your physical \"forward\" model that takes in a vector with these three parameters:\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Orbits\nusing Transits\n\nt0_i = 1.311544\nb_i = 0.1\nrp_i = 0.01\n\ncreate_orbit(t0, b; a=15, per=3, kw...) = KeplerianOrbit(; period=per, t0, a, b, kw...)\ncreate_orbit(t::Tuple; kw...) = create_orbit(t[1], t[2]; kw...)\n\nfunction light_curve(time, t0, b, rp; u=[0.3, 0.3], kwargs...)\n    orbit = create_orbit(t0, b; kwargs...)\n    ld = QuadLimbDark(u)\n    return @. ld(orbit, time, rp)\nend\n\nflux = light_curve(data.time, t0_i, b_i, rp_i)\nscatterlines!(f[1, 1], data.time, flux; markersize)\nf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-4-output-1.svg){}\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nfunction light_curve_n(time, params::NTuple{N}; ld=QuadLimbDark([0.3, 0.3]), kwargs...) where {N}\n    rps = getindex.(params, 3)\n    orbits = create_orbit.(params; kwargs...)\n    map(time) do t\n        s = reduce(+, ntuple(i -> ld(orbits[i], t, rps[i]), N))\n        s - (N - 1) * one(s)\n    end\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nlight_curve_n (generic function with 1 method)\n```\n:::\n:::\n\n\n\n### Step 2: Define the Bayesian analysis \n\n> 1. Write the given prior information as a log-prior function\n> 2. Write a log-likelihood function\n> 3. Write a log-probability function\n\nVanilla log-prior, log-likelihood, and log-probability functions could be written as:\n\n```julia\nlog_prior(x, (lb, ub)) = lb <= x <= ub ? 0.0 : -Inf\n\n# Transit time between 1.212 and 1.362 days\n# Planet radius between 1% and 10% of star radius\n# Impact parameter between 0 and 1\nlog_prior(t0, rp, b) = log_prior(t0, (1.212, 1.362)) + \n    log_prior(rp, (0.01, 0.10)) + \n    log_prior(b, (0.0, 1.0))\n\nfunction log_likelihood(params, time, obs, error;\n    per=3.0, # Period in days\n    a=15.0, # Semi-major axis in stellar radii\n    u=[0.3, 0.3] # Limb darkening coefficients\n)\n    t0, rp, b = params\n    inc = acosd(b / a)  # Inclination in degrees\n    model_flux = light_curve(time; t0, per, rp, a, inc, u)\n\n    # Calculate log-likelihood (assuming Gaussian errors)\n    residuals = obs .- model_flux\n    chi_squared = sum((residuals ./ error) .^ 2)\n    -0.5 * chi_squared\nend\n\nlog_probability(params, time, obs, error) = log_prior(params) + log_likelihood(params, time, obs, error)\n```\n\nUsing [`Turing.jl`](https://github.com/TuringLang/Turing.jl), we can write the same model more compactly:\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Turing\n\n@model function transit_model(time, flux; error=1e-4)\n    t0 ~ Uniform(1.212, 1.362)\n    rp ~ Uniform(0.01, 0.10)\n    b ~ Uniform(0.0, 0.8)\n    model_flux = light_curve(time, t0, b, rp)\n    return flux ~ MvNormal(model_flux, error)\nend\n\nmodel = transit_model(data.time, data.flux)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nDynamicPPL.Model{typeof(transit_model), (:time, :flux), (:error,), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{Float64}, DynamicPPL.DefaultContext}(transit_model, (time = [1.1620000000000001, 1.162836120401338, 1.1636722408026758, 1.1645083612040135, 1.1653444816053513, 1.1661806020066892, 1.167016722408027, 1.1678528428093646, 1.1686889632107025, 1.1695250836120403  …  1.40447491638796, 1.4053110367892978, 1.4061471571906357, 1.4069832775919733, 1.4078193979933111, 1.408655518394649, 1.4094916387959868, 1.4103277591973247, 1.4111638795986623, 1.4120000000000001], flux = [0.99989143693967, 1.0000997345446583, 1.0000282978498052, 0.9998493705286082, 0.9999421399748032, 1.0001651436537098, 0.9997573320756606, 0.9999571087371144, 1.0001265936258705, 0.9999133259597734  …  0.9998925233406881, 1.000066831686674, 1.0000955832355167, 0.9999122386413373, 0.9998076284270303, 1.000069578731914, 1.0001875800546713, 1.0000415694539893, 1.000016054442148, 1.000081976060961]), (error = 0.0001,), DynamicPPL.DefaultContext())\n```\n:::\n:::\n\n\n\n### Step 3: Demonstrate solution on a grid \n\n> Determine new estimates with uncertainties of the planet-to-star radius ratio and the impact parameter. Fix the transit mid-time to t0 = 1.311544 days. Make a 50x50 grid that explores planet-to-star radius ratios between 0.05275 and 0.53505 as well as impact parameters between 0 and 0.3.\n\n#### 1. Plot the joint posterior distribution with and mark the best fit with a marker\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing LinearAlgebra  # For eigen decomposition and other matrix operations\nusing Statistics     # For statistical functions\nusing StatsBase\n\nfixed_t0 = 1.311544\n\nn_grid = 50\nrp_grid = range(0.05275, 0.053505, length=n_grid)\nb_grid = range(0.0, 0.3, length=n_grid - 10)\n\nlog_posterior_grid = [\n    logjoint(model, (; rp, b, t0=fixed_t0))\n    for rp in rp_grid, b in b_grid\n]\n\nposterior_grid = exp.(log_posterior_grid .- maximum(log_posterior_grid)) # (subtract max to avoid numerical issues)\nposterior_grid = posterior_grid ./ sum(posterior_grid) # Normalize the posterior\n\n# Find the maximum posterior point\nmax_idx = argmax(posterior_grid)\nbest_rp = rp_grid[max_idx[1]]\nbest_b = b_grid[max_idx[2]]\n@info \"Best fit values\" best_rp best_b\n\nb_label = \"Impact parameter (b)\"\nrp_label = \"Planet-to-star radius ratio (rp)\"\nfig = Figure(size=(1000, 800))\nax1 = Axis(fig[1, 1], xlabel=rp_label, ylabel=b_label, title=\"Joint Posterior Distribution\")\n\nhm = heatmap!(ax1, rp_grid, b_grid, posterior_grid)\nscatter!(ax1, [best_rp], [best_b], color=:red, markersize=15)\nColorbar(fig[1, 1, Right()], hm, label=\"Posterior Probability\")\nfig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Best fit values\n│   best_rp = 0.053150612244897956\n└   best_b = 0.2076923076923077\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-7-output-2.svg){}\n:::\n:::\n\n\n\n#### 2. Marginalized posterior distributions \n\n> Plot the marginalized posterior distributions for the planet-to-star radius ratio and the impact parameters.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nrp_posterior = vec(sum(posterior_grid, dims=2))\nrp_posterior = weights(rp_posterior ./ sum(rp_posterior))  # Normalize\n\nb_posterior = vec(sum(posterior_grid, dims=1))\nb_posterior = weights(b_posterior ./ sum(b_posterior))  # Normalize\n\nax2 = Axis(fig[2, 1], xlabel=rp_label, ylabel=\"Probability\")\nlines!(ax2, rp_grid, rp_posterior)\nax3 = Axis(fig[1, 2], xlabel=b_label, ylabel=\"Probability\")\nlines!(ax3, b_grid, b_posterior)\nfig\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-8-output-1.svg){}\n:::\n:::\n\n\n\n#### 3. Best estimates and uncertainties\n\n> Calculate the mean and +/-1 sigma uncertainties for both parameters. Mark the mean value with a solid vertical line in the figure created in Step 3.2. Make the +/-1 sigma uncertainties with dashed vertical lines. Report the best estimates (mean and uncertainty) for both parameters quantitatively.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Measurements\n# For rp\nrp_mean = mean(rp_grid, rp_posterior)\nrp_std = std(rp_grid, rp_posterior)\nrp = measurement(rp_mean, rp_std)\n\n# For b\nb_mean = mean(b_grid, b_posterior)\nb_std = std(b_grid, b_posterior)\nb = measurement(b_mean, b_std)\n\n# Add mean and +/- 1 sigma to the marginalized plots\nvlines!(ax2, [rp_mean], color=:black, linewidth=2)\nvlines!(ax2, [rp_mean - rp_std, rp_mean + rp_std], color=:black, linestyle=:dash)\n\nvlines!(ax3, [b_mean], color=:black, linewidth=2)\nvlines!(ax3, [b_mean - b_std, b_mean + b_std], color=:black, linestyle=:dash)\n\n@info \"Planet-to-star radius ratio\" rp\n@info \"Impact parameter\" b\nfig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Planet-to-star radius ratio\n└   rp = 0.05313 ± 0.00011\n┌ Info: Impact parameter\n└   b = 0.204 ± 0.0089\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-9-output-2.svg){}\n:::\n:::\n\n\n\n#### 4. Covariance matrix and confidence regions\n\n> Calculate the covariance matrix that best represents the joint posterior distribution. What is the correlation coefficient between the planet-to-star radius ratios and the impact parameter? Plot the 68% and 95% confidence regions onto the figure created in Step 3.1.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\n\"\"\"Calculate covariance matrix elements\"\"\"\nfunction covariance_matrix(p, x, y)\n    x_2d = repeat(x, 1, length(y))\n    y_2d = repeat(y', length(x), 1)\n    wp = weights(p)\n    x_mean = mean(x_2d, wp)\n    y_mean = mean(y_2d, wp)\n\n    # Calculate covariance matrix elements\n    cov_x_x = mean((x_2d .- x_mean) .^ 2, wp)\n    cov_y_y = mean((y_2d .- y_mean) .^ 2, wp)\n    cov_x_y = mean((x_2d .- x_mean) .* (y_2d .- y_mean), wp)\n    return [cov_x_x cov_x_y; cov_x_y cov_y_y]\nend\n\ncov_matrix = covariance_matrix(posterior_grid, rp_grid, b_grid)\n@info \"Covariance matrix:\" cov_matrix\n\n# Calculate correlation coefficient\ncorr_coef = cov_matrix[1, 2] / (sqrt(cov_matrix[1, 1]) * sqrt(cov_matrix[2, 2]))\n@info \"Correlation coefficient:\" corr_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Covariance matrix:\n│   cov_matrix =\n│    2×2 Matrix{Float64}:\n│     1.22606e-8  3.77043e-7\n└     3.77043e-7  7.99982e-5\n┌ Info: Correlation coefficient:\n└   corr_coef = 0.3807109735554177\n```\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Distributions\n\nfunction getellipsepoints(cx, cy, rx, ry, θ; length=100)\n    t = range(0, 2π; length)\n    ellipse_x_r = @. rx * cos(t)\n    ellipse_y_r = @. ry * sin(t)\n    R = [cos(θ) sin(θ); -sin(θ) cos(θ)]\n    r_ellipse = [ellipse_x_r ellipse_y_r] * R\n    x = @. cx + r_ellipse[:, 1]\n    y = @. cy + r_ellipse[:, 2]\n    (x, y)\nend\n\nfunction getellipsepoints(μ, Σ, confidence=0.95)\n    quant = quantile(Chisq(2), confidence) |> sqrt\n\n    egvs = eigvals(Σ)\n    if egvs[1] > egvs[2]\n        idxmax = 1\n        largestegv = egvs[1]\n        smallesttegv = egvs[2]\n    else\n        idxmax = 2\n        largestegv = egvs[2]\n        smallesttegv = egvs[1]\n    end\n\n    rx = quant * sqrt(largestegv)\n    ry = quant * sqrt(smallesttegv)\n\n    eigvecmax = eigvecs(Σ)[:, idxmax]\n    θ = atan(eigvecmax[2] / eigvecmax[1])\n    if θ < 0\n        θ += 2π\n    end\n    getellipsepoints(μ..., rx, ry, θ)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\ngetellipsepoints (generic function with 3 methods)\n```\n:::\n:::\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nμ = [rp_mean, b_mean]\nΣ = cov_matrix\n\nlines!(ax1, getellipsepoints(μ, Σ)..., label=\"95% confidence interval\")\nlines!(ax1, getellipsepoints(μ, Σ, 0.5)..., label=\"50% confidence interval\")\nfig\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-12-output-1.svg){}\n:::\n:::\n\n\n\n## Part B\n \n### Step 4: Solve the full problem using MCMC and Nested Sampling \n\n> Determine new estimates with uncertainties of the transit time, the planet-to-star radius ratio, and the impact parameter.\n> 1. Using Emcee. Make all possible plots and quantitative assessments to convince the reviewer that your results are converged \n> 2. Using Dynesty. Make all possible plots and quantitative assessments to convince the reviewer  that your results are converged  \n\n#### Affine Invariant MCMC (Emcee-like)\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nchain = sample(model, Emcee(20), MCMCThreads(), 1000, 4; progress=false)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nChains MCMC chain (1000×4×80 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 80\nSamples per chain = 1000\nparameters        = t0, rp, b\ninternals         = lp\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n          t0    1.3055    0.0244    0.0018   454.9317   193.9590    1.2169     ⋯\n          rp    0.0509    0.0121    0.0009   390.4781   204.4670    1.2722     ⋯\n           b    0.2354    0.1386    0.0097   383.4920   190.2810    1.2797     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n          t0    1.2120    1.3115    1.3115    1.3116    1.3121\n          rp    0.0102    0.0530    0.0531    0.0532    0.0561\n           b    0.0002    0.1985    0.2057    0.2136    0.7988\n```\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing StatsPlots\nStatsPlots.plot(chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-14-output-1.svg){}\n:::\n:::\n\n\n\n#### NestedSamplers (Dynesty-like)\n\n> [chalk-lab/NestedSamplers.jl: Implementations of single and multi-ellipsoid nested sampling](https://github.com/chalk-lab/NestedSamplers.jl)\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing NestedSamplers\n\nloglike = let m = model\n    (X) -> loglikelihood(m, (t0=X[1], rp=X[2], b=X[3]))\nend\n\npriors = [Uniform(1.212, 1.362), Uniform(0.01, 0.10), Uniform(0.0, 0.8)]\nnmodel = NestedModel(loglike, priors)\n\nsampler = Nested(3, 2000)\nchain, state = sample(nmodel, sampler, progress=false)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(MCMC chain (36277×4×1 Array{Float64, 3}), (it = 36277, us = [0.6633419982972202 0.6634610489768484 … 0.6636630535044208 0.6635219756683381; 0.4794670093953377 0.48008582122653015 … 0.47904932981186515 0.4782377822234889; 0.2638313976733687 0.2648857712374485 … 0.26095956561724315 0.2441352507852346], vs = [1.3115012997445832 1.3115191573465272 … 1.3115494580256633 1.3115282963502508; 0.0531520308455804 0.05320772391038772 … 0.05311443968306787 0.05304140040011401; 0.211065118138695 0.2119086169899588 … 0.20876765249379453 0.19530820062818768], logl = 2232.7468670304697, logz = 2217.0932433724383, logzerr = 0.08609346808799914, logvol = -24.739402459542085, since_update = 924, has_bounds = true, active_bound = MultiEllipsoid{Float64}(ndims=3)))\n```\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nStatsPlots.plot(chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-16-output-1.svg){}\n:::\n:::\n\n\n\n#### Hamiltonian Monte Carlo (HMC)\n\nSince now the light curve is auto-differentiated, we can use algorithms like `No U-Turn Sampler (NUTS)` which uses Hamiltonian dynamics (very efficient).\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nchain = sample(model, NUTS(), 2000, progress=false)\nStatsPlots.plot(chain)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-17-output-2.svg){}\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing PairPlots: pairplot\npairplot(chain)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-18-output-1.svg){}\n:::\n:::\n\n\n\n#### Performance comparison\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Chairmarks\ndisplay(@b sample(model, NUTS(), 2000; progress=false))\ndisplay(@b sample(model, Emcee(16), 2000; progress=false))\ndisplay(@b sample(nmodel, sampler, progress=false))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n1.387 s (11860669 allocs: 1.100 GiB, 7.82% gc time, without a warmup)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n2.716 s (15638912 allocs: 1.159 GiB, 4.87% gc time, 17.98% compile time, without a warmup)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n34.764 s (49698346 allocs: 11.488 GiB, 6.78% gc time, without a warmup)\n```\n:::\n:::\n\n\n\nWe can see that NUTS is the fastest, followed by Emcee, and then NestedSamplers. Also NUTS seems to converge faster than Emcee and NestedSamplers.\n\n### Step 5: Investigate whether you model is a good fit to the data \n\n> Investigate the residuals. What do you think? Quantitatively and graphically assess whether your model is a good fit to the data. \n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Statistics\n\nt0_fit = median(chain[:t0])\nrp_fit = median(chain[:rp])\nb_fit = median(chain[:b])\nmodel_flux = light_curve(data.time, t0_fit, b_fit, rp_fit)\nresiduals = data.flux .- model_flux\n\nf1 = Figure()\nmarkersize = 4\nscatterlines(f1[1, 1], data.time, data.flux; markersize)\nscatterlines!(f1[1, 1], data.time, model_flux; markersize)\n\nAxis(f1[2, 1], xlabel=\"Time\", ylabel=\"Residuals\", title=\"Residuals vs. Time\")\nscatter!(f1[2, 1], data.time, residuals)\nAxis(f1[3, 1], xlabel=\"Residual\", ylabel=\"Count\", title=\"Histogram of Residuals\")\nhist!(f1[3, 1], residuals, bins=30)\ndisplay(f1)\n\n@info \"Mean residual: \" mean(residuals)\n@info \"Std of residuals: \" std(residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Mean residual: \n└   mean(residuals) = -8.141298699683164e-6\n┌ Info: Std of residuals: \n└   std(residuals) = 0.00013008406925382225\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](proj1_files/figure-typst/cell-20-output-2.svg){}\n:::\n:::\n\n\n\n### Step 6: Refine your physical model \n\n> 1. What could explain your findings in Part 4? Tip: When a distant observer sees a transit of the Earth in front of the sun, would it only be the Earth that is transiting at that time? \n\nThere is a increase in the residuals at the time of the $1.33$. This could be due to the presence of another object transiting the star. For example, if the system contains both a planet and a moon (or another planet), the observed light curve would show additional dips or asymmetries not explained by a single-object model.\n\n> 2. Code a new physical \"forward\" model and log-likelihood function that better explains the data.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\n@model function transit_model_two_bodies(time, flux; error=1e-4)\n    # Priors for planet\n    t0_1 ~ Uniform(1.212, 1.362)\n    rp_1 ~ Uniform(0.01, 0.10)\n    b_1 ~ Uniform(0.0, 0.8)\n    # Priors for moon (or second planet)\n    t0_2 ~ Uniform(1.212, 1.362)\n    rp_2 ~ Uniform(0.01, 0.10)\n    b_2 ~ Uniform(0.0, 0.8)\n\n    model_flux = light_curve_n(time, ((t0_1, b_1, rp_1), (t0_2, b_2, rp_2)))\n    return flux ~ MvNormal(model_flux, error)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\ntransit_model_two_bodies (generic function with 2 methods)\n```\n:::\n:::\n\n\n\n> 3. Use Dynesty to find the constraints of both objects. Treat the moon like another body transiting shortly before or after. Use the same prior for the moon. That makes six parameters.\n\nFor performance reason, we use `NUTS` instead of `NestedSamplers`.\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nmodel2 = transit_model_two_bodies(data.time, data.flux)\nchain2 = sample(model2, NUTS(), 3000, progress=false)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nChains MCMC chain (3000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:4000\nNumber of chains  = 1\nSamples per chain = 3000\nWall duration     = 23.63 seconds\nCompute duration  = 23.63 seconds\nparameters        = t0_1, rp_1, b_1, t0_2, rp_2, b_2\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n        t0_1    1.3021    0.0003    0.0000   2143.2478   1936.2447    0.9998   ⋯\n        rp_1    0.0200    0.0008    0.0000   1300.3433   1725.2654    1.0005   ⋯\n         b_1    0.5087    0.0170    0.0003   2650.6522   1673.3757    1.0009   ⋯\n        t0_2    1.3120    0.0001    0.0000   2488.5882   2421.0591    1.0000   ⋯\n        rp_2    0.0501    0.0003    0.0000   1376.5892   1835.7400    1.0007   ⋯\n         b_2    0.2142    0.0089    0.0002   2677.1743   1705.1781    1.0005   ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n        t0_1    1.3015    1.3019    1.3021    1.3023    1.3028\n        rp_1    0.0185    0.0195    0.0200    0.0206    0.0215\n         b_1    0.4747    0.4974    0.5091    0.5200    0.5419\n        t0_2    1.3119    1.3120    1.3120    1.3121    1.3121\n        rp_2    0.0496    0.0499    0.0501    0.0503    0.0506\n         b_2    0.1961    0.2086    0.2145    0.2201    0.2307\n```\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nStatsPlots.plot(chain2) |> display\npairplot(chain2)\n```\n\n::: {.cell-output .cell-output-display}\n![](proj1_files/figure-typst/cell-23-output-1.svg){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-23-output-2.svg){}\n:::\n:::\n\n\n\n> 4. Make plots to convince the reviewer that your results are converged  \n\nThe figure below shows the trace and autocorrelation plots for all parameters. The chains appear well-mixed and stationary, indicating convergence. The Monte Carlo standard error is small further supporting convergence.\n\nReference: [Autocorrelation analysis & convergence — emcee](https://emcee.readthedocs.io/en/stable/tutorials/autocorr/)\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\n@info \"Monte Carlo standard error\" mcse(chain2)\nautocorplot(chain2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Monte Carlo standard error\n│   mcse(chain2) =\n│    MCSE\n│      parameters      mcse \n│          Symbol   Float64 \n│    \n│            t0_1    0.0000\n│            rp_1    0.0000\n│             b_1    0.0003\n│            t0_2    0.0000\n│            rp_2    0.0000\n│             b_2    0.0002\n└    \n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](proj1_files/figure-typst/cell-24-output-2.svg){}\n:::\n:::\n\n\n\n> 5. Determine quantitatively whether the model justifies applying this more complex model? How confident are you that you detected a moon?\n\nHere we do a posterior predictive checks to compare how well each model reproduces the observed data\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nt0_1 = median(chain2[:t0_1])\nrp_1 = median(chain2[:rp_1])\nb_1 = median(chain2[:b_1])\nt0_2 = median(chain2[:t0_2])\nrp_2 = median(chain2[:rp_2])\nb_2 = median(chain2[:b_2])\nmodel_flux2 = light_curve_n(data.time, ((t0_1, b_1, rp_1), (t0_2, b_2, rp_2)))\nresiduals2 = data.flux .- model_flux2\n\nscatterlines!(f1[1, 1], data.time, model_flux2; markersize)\nscatter!(f1[2, 1], data.time, residuals2)\nhist!(f1[3, 1], residuals2, bins=30)\ndisplay(f1)\n\n@info \"Mean residual: \" mean(residuals2)\n@info \"Std of residuals: \" std(residuals2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n┌ Info: Mean residual: \n└   mean(residuals2) = -3.4637321423132877e-6\n┌ Info: Std of residuals: \n└   std(residuals2) = 0.00010219966645211133\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](proj1_files/figure-typst/cell-25-output-2.svg){}\n:::\n:::\n\n\n\nThe two-body yields lower residuals than the single-body model, indicating a better fit.\n\n6. Summarize your results for the parameters for each object? \n\nThe summary statistics for the two-body model are shown below:\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nchain2\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nChains MCMC chain (3000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:4000\nNumber of chains  = 1\nSamples per chain = 3000\nWall duration     = 23.63 seconds\nCompute duration  = 23.63 seconds\nparameters        = t0_1, rp_1, b_1, t0_2, rp_2, b_2\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n        t0_1    1.3021    0.0003    0.0000   2143.2478   1936.2447    0.9998   ⋯\n        rp_1    0.0200    0.0008    0.0000   1300.3433   1725.2654    1.0005   ⋯\n         b_1    0.5087    0.0170    0.0003   2650.6522   1673.3757    1.0009   ⋯\n        t0_2    1.3120    0.0001    0.0000   2488.5882   2421.0591    1.0000   ⋯\n        rp_2    0.0501    0.0003    0.0000   1376.5892   1835.7400    1.0007   ⋯\n         b_2    0.2142    0.0089    0.0002   2677.1743   1705.1781    1.0005   ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n        t0_1    1.3015    1.3019    1.3021    1.3023    1.3028\n        rp_1    0.0185    0.0195    0.0200    0.0206    0.0215\n         b_1    0.4747    0.4974    0.5091    0.5200    0.5419\n        t0_2    1.3119    1.3120    1.3120    1.3121    1.3121\n        rp_2    0.0496    0.0499    0.0501    0.0503    0.0506\n         b_2    0.1961    0.2086    0.2145    0.2201    0.2307\n```\n:::\n:::\n\n\n\n7. Why do you get a multi-modal solution? What could you do to avoid this in this simple example? \n\nIt seems that using `NUTS` sampler with only one chain could avoid the multi-modal solution. Multi-modal arises when there are multiple chains and the modes switch between chains.\n\n> This is because it’s possible for either model parameter to be assigned to either of the corresponding true means, and this assignment need not be consistent between chains. That is, the posterior is fundamentally multimodal, and different chains can end up in different modes, complicating inference. One solution here is to enforce an ordering on our parameters, requiring for all. `Bijectors.jl` provides an easy transformation (ordered()) for this purpose\n\nAnother solution is to post-process the chain to exchange parameters to enforce ordering.\n\n### Step 7: \n\n> Describe in a few sentences one example for a problem or data set that you could analogously solve in your research domain? \n\nIn plasma physics, particle velocity distributions are often non-Maxwellian and may be composed of several populations (core, halo, beam). By modeling the observed distribution as the sum of several Maxwellian or kappa distributions, and using Bayesian inference to fit the parameters (density, temperature, drift speed for each component), you can determine how many components are justified and estimate their properties, just as you determine the number of transiting bodies in the exoplanet problem.\n\n## Old codes (using `Batman`)\n\n\n### Julia interface to Python and Batman\n\n```julia\n#| output: false\nusing CondaPkg\nusing PythonCall\nimport PythonCall: Py\npkg = \"batman-package\"\ndeps = CondaPkg.read_deps()[\"deps\"]\nhaskey(deps, pkg) || CondaPkg.add(pkg)\n# CondaPkg.add(\"corner\")\n\nconst batman = pyimport(\"batman\")\nconst corner = pyimport(\"corner\")\nconst np = pyimport(\"numpy\")\n```\n\n```julia\nusing DrWatson: @dict\n\nfunction set_params!(params; kwargs...)\n    for (key, value) in kwargs\n        setproperty!(params, key, value)\n    end\n    return params\nend\n\n\"\"\"Like `batman.TransitParams` in Python, but with some keyword arguments for convenience.\"\"\"\nfunction TransitParams(; ecc=0.0, w=0.0, limb_dark=\"quadratic\", kwargs...)\n    params = batman.TransitParams()\n    set_params!(params; ecc, w, limb_dark)\n    set_params!(params; kwargs...)\n    return params\nend\n\nTransitModel(params, time) = batman.TransitModel(params, Py(time).to_numpy())\n\nlight_curve(model::Py, params::Py) = PyArray(model.light_curve(params))\nfunction light_curve(params, time)\n    m = TransitModel(params, time)\n    return light_curve(m, params)\nend\nlight_curve(time; kwargs...) = light_curve(TransitParams(; kwargs...), time)\n```\n\n```julia\ninc(a, b) = acosd(b / a)\ninc_i = acosd(b_i / a_i)\n\nflux = let rp = rp_i, t0 = t0_i, b = b_i, a = a_i, u = u_i, per = per_i\n    inc = acosd(b / a)\n    light_curve(data.time; t0, per, rp, a, inc, u)\nend\n```\n\nEmcee sampler\n\n```julia\ninc(p::Py, b) = inc(pyconvert(Any, p.a), b)\n\n@model function transit_model(time, flux, params, trans_model=TransitModel(params, data.time); error=1e-4)\n    # Priors\n    t0 ~ Uniform(1.212, 1.362)\n    rp ~ Uniform(0.01, 0.10)\n    b ~ Uniform(0.0, 0.99)\n    params = set_params!(params; t0, rp, inc=inc(params, b))\n    model_flux = light_curve(trans_model, params)\n    return flux ~ MvNormal(model_flux, error)\nend\n\nparams = TransitParams(; rp=rp_i, t0=t0_i, per=3.0, a=a_i, u=u_i, inc=inc_i)\ntrans_model = TransitModel(params, data.time)\nmodel = transit_model(data.time, data.flux, params)\nchain = sample(model, Emcee(100), 10000; progress=false)\n```\n\n\n## Reference\n\n- [DACE tutorial on photometry](https://dace.unige.ch/tutorials/?tutorialId=32)\n\n- Dynamic Hamiltonian Monte Carlo (HMC)\n    - [tpapp/DynamicHMC.jl: Implementation of robust dynamic Hamiltonian Monte Carlo methods (NUTS) in Julia](https://github.com/tpapp/DynamicHMC.jl)\n\n",
    "supporting": [
      "proj1_files/figure-typst"
    ],
    "filters": []
  }
}